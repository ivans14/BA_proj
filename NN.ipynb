{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, precision_score\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exp=pd.read_csv('BikeSharing_Bluebikes2022.csv')\n",
    "nat_hol = ['2022-01-01','2022-01-17','2022-02-21','2022-04-17','2022-04-18','2022-05-08','2022-05-30','2022-06-19','2022-06-20','2022-07-04']\n",
    "\n",
    "data_exp['is_Holiday'] = [ 1 if data_exp.iloc[i]['starttime'][0:10] in nat_hol else 0 for i in range(len(data_exp))]\n",
    "data_w = pd.read_csv(\"Boston 2022-01-01 to 2022-08-31.csv\")\n",
    "drop = ['dew', 'sunrise','sunset','moonphase','conditions', 'description','stations','sealevelpressure',\n",
    "'windgust','severerisk', 'uvindex', 'solarenergy', 'solarradiation','preciptype','winddir','name']\n",
    "data_w = data_w.drop(columns = drop)\n",
    "data_w['datetime'] = pd.to_datetime(data_w['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "data_exp['starttime'] = pd.to_datetime(data_exp['starttime'], format='%Y-%m-%d %H:%M:%S')\n",
    "data_exp['datetime'] = pd.to_datetime(data_exp['starttime']).dt.date\n",
    "data_exp['datetime'] = pd.to_datetime(data_exp['datetime'])\n",
    "\n",
    "data_merge = data_exp.merge(data_w, how='left', on='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merge.columns\n",
    "data_merge = data_merge.drop(columns=(['Unnamed: 0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tripduration', 'starttime', 'stoptime', 'start station id',\n",
       "       'start station name', 'start station latitude',\n",
       "       'start station longitude', 'end station id', 'end station name',\n",
       "       'end station latitude', 'end station longitude', 'bikeid', 'usertype',\n",
       "       'postal code', 'is_Holiday', 'datetime', 'tempmax', 'tempmin', 'temp',\n",
       "       'feelslikemax', 'feelslikemin', 'feelslike', 'humidity', 'precip',\n",
       "       'precipprob', 'precipcover', 'snow', 'snowdepth', 'windspeed',\n",
       "       'cloudcover', 'visibility', 'icon'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merge.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([178, 189,  94,  19, 107,  36,  58,  60,   4, 377, 342, 491, 362,\n",
       "        43, 121, 381,  10,  81, 332, 328, 233, 176,  55,  80,  12,  68,\n",
       "        32, 202,  54,  74,  39,  66, 356, 116, 398,  42, 352, 330, 426,\n",
       "        67, 225, 234,  91,  76, 161, 386, 374, 345, 437, 487,  40, 415,\n",
       "       149, 183,  23,  71,   6,  72, 131, 118,  87,  86,  25,  97, 119,\n",
       "       338,  69, 359, 446, 239,  22,  47,  26, 125,  33, 157, 105, 471,\n",
       "        41, 174,  30,  16, 400, 177, 191, 152, 378, 218, 179, 403,  75,\n",
       "       443, 379,  17,  46,   7,  11, 226, 363, 479, 190, 536, 141,  78,\n",
       "       181, 139, 380, 417,  95,  90, 364, 549, 529,  44, 151, 376, 100,\n",
       "        24, 137, 104, 389,  20,  96, 108,  98, 272, 187,  21, 541, 370,\n",
       "        59, 393, 163, 413,  65, 215, 175, 414, 186, 318,  93, 344, 554,\n",
       "       472,  63, 169, 196,  51, 419, 331,  84, 478, 515, 115, 213, 372,\n",
       "       361, 192,  31, 136, 197, 138, 110, 273,  37,  49, 228, 456,  73,\n",
       "       206,  70, 407, 194, 193, 440,   9, 480, 102, 327,   8,   3,  99,\n",
       "       530, 180, 544, 184, 447,  29,  89, 346, 195, 235, 405, 399, 404,\n",
       "       143, 212, 365, 553, 462, 334, 470, 279, 159, 452, 387, 408, 126,\n",
       "       217, 390, 162, 358, 103, 385, 117, 550, 333, 495, 507, 475, 280,\n",
       "       282, 205, 445, 441, 547, 208, 433, 483,  85, 351, 201, 531,  77,\n",
       "       203, 109, 171,  82, 185, 353, 214, 200, 319, 397, 424, 221, 336,\n",
       "        56, 150, 548, 488, 271, 525, 146, 170, 188, 458, 335,  15, 455,\n",
       "       182, 463, 461, 533, 509, 222, 360, 502, 486,  92, 539, 224, 401,\n",
       "       371, 329, 236, 489, 101, 296, 492,  14, 124, 348, 142, 394, 540,\n",
       "       350, 156, 135, 482, 442, 160, 140, 395, 523, 133, 111, 145, 402,\n",
       "       258, 219, 508, 367, 425, 494, 532, 505, 481, 528, 518, 337, 355,\n",
       "       506, 520, 173, 485, 535, 493, 466, 144, 473, 341, 340, 349, 522,\n",
       "       503, 391, 392, 469, 373, 432, 519, 499, 497, 496, 467, 436, 490,\n",
       "       545, 434, 527, 232, 422, 538, 498, 537, 210, 468, 448, 396,  27,\n",
       "       557, 526, 524, 555, 521, 546, 504, 474, 558, 484, 561, 112, 409,\n",
       "       406, 560, 556,  79, 114,  13, 459, 460, 354, 428, 516, 517, 421,\n",
       "       477, 559, 423, 255, 347, 513, 562, 416, 260, 384,   1, 339, 343,\n",
       "       430, 412, 427, 511,  35, 512, 465, 510, 464, 410,  57, 368, 542,\n",
       "       129,  53, 211,  64,  48, 227, 122, 357, 204, 128, 167, 130, 199,\n",
       "       543, 216,  50,  52, 454,   5, 120, 113,  61, 435, 259, 431, 563,\n",
       "       317, 106, 564, 565, 308, 566, 514, 567, 568, 569, 570, 572, 571],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merge['start station id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49127"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = []\n",
    "for i in data_merge['start station id'].unique():\n",
    "    count = data_merge[data_merge['start station id']==i].count()[0]\n",
    "    list.append(count)\n",
    "\n",
    "maxim = data_merge['start station id'].unique()[list.index(max(list))]\n",
    "print(maxim)\n",
    "data_merge[data_merge['start station id']==maxim].count()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tripduration', 'starttime', 'stoptime', 'start station id',\n",
       "       'start station name', 'start station latitude',\n",
       "       'start station longitude', 'end station id', 'end station name',\n",
       "       'end station latitude', 'end station longitude', 'bikeid', 'usertype',\n",
       "       'postal code', 'is_Holiday', 'datetime', 'tempmax', 'tempmin', 'temp',\n",
       "       'feelslikemax', 'feelslikemin', 'feelslike', 'humidity', 'precip',\n",
       "       'precipprob', 'precipcover', 'snow', 'snowdepth', 'windspeed',\n",
       "       'cloudcover', 'visibility', 'icon'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merge.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merge = data_merge[data_merge['start station id']==maxim]\n",
    "data_nn = data_merge.drop(columns=['start station id',\n",
    "       'start station name', 'start station latitude',\n",
    "       'start station longitude','end station name',\n",
    "       'end station latitude', 'end station longitude','bikeid','postal code', 'stoptime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran = data_nn['starttime'].iloc[3]\n",
    "ran.hour\n",
    "(data_nn['starttime'].iloc[0].hour >= 0) & (data_nn['starttime'].iloc[0].hour < 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mirun\\AppData\\Local\\Temp\\ipykernel_18588\\3059181073.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_nn['tripduration'].iloc[0]=2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(data_nn))\n",
    "# data_nn['tripduration'].iloc[0]=2\n",
    "# data_nn['tripduration'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mirun\\AppData\\Local\\Temp\\ipykernel_18588\\3777789447.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_nn['time_of_day'].iloc[i] = 'night'\n"
     ]
    }
   ],
   "source": [
    "data_nn['time_of_day'] = 0\n",
    "for i in range(len(data_nn)):\n",
    "    if (data_nn['starttime'].iloc[i].hour >= 6) & (data_nn['starttime'].iloc[i].hour < 12):\n",
    "        data_nn['time_of_day'].iloc[i] = 'morning'\n",
    "    elif (data_nn['starttime'].iloc[i].hour >= 12) & (data_nn['starttime'].iloc[i].hour < 18):\n",
    "        data_nn['time_of_day'].iloc[i] = 'afternoon'\n",
    "    elif (data_nn['starttime'].iloc[i].hour >= 18) & (data_nn['starttime'].iloc[i].hour < 24):\n",
    "        data_nn['time_of_day'].iloc[i] = 'evening'\n",
    "    elif (data_nn['starttime'].iloc[i].hour >= 0) & (data_nn['starttime'].iloc[i].hour < 6):\n",
    "        data_nn['time_of_day'].iloc[i] = 'night'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tripduration', 'end station id', 'usertype', 'is_Holiday', 'tempmax',\n",
       "       'tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike',\n",
       "       'humidity', 'precip', 'precipprob', 'precipcover', 'snow', 'snowdepth',\n",
       "       'windspeed', 'cloudcover', 'visibility', 'icon', 'time_of_day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nn = data_nn.drop(columns=['starttime','datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nn.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nn_dummies = pd.get_dummies(data_nn, columns=['time_of_day', 'icon', 'usertype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nn_dummies_std = data_nn_dummies.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_std = ['tripduration', 'tempmax',\n",
    "       'tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike',\n",
    "       'humidity', 'precip', 'precipprob', 'precipcover', 'snow', 'snowdepth',\n",
    "       'windspeed', 'cloudcover', 'visibility']\n",
    "\n",
    "data_nn_dummies_std[col_std]= (data_nn_dummies_std[col_std] - data_nn_dummies_std[col_std].mean()) / data_nn_dummies_std[col_std].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 67, 139,  55,  36, 471, 108,  68, 178,  60,  75, 107, 372, 224,\n",
       "        46, 115,  97,  91, 221, 377, 370,  11, 544, 515, 386,  76, 342,\n",
       "        74, 352, 184,  44,  72, 105, 380, 361, 189,  98,  80, 185, 141,\n",
       "       179,  95,  33,  84, 180,  89,   9, 408,  99, 177,  90, 335,  21,\n",
       "        25, 338,  41, 413, 472,  54,  32, 381, 437, 549, 228,  42, 462,\n",
       "       479,   6,  10, 190, 330, 374, 318,  16,  22, 364, 100,  39, 400,\n",
       "       404, 116, 160, 193, 176, 417,  73, 110, 378, 399,   3, 143, 403,\n",
       "       104, 225,  19,  12, 446,  86,  51, 233,  94, 332, 401,  66, 379,\n",
       "       206,  81, 226, 554,  87, 333, 376, 398, 205, 131, 499,  37, 426,\n",
       "       117, 553,  70, 345,  78, 234,  14, 195, 138,  43, 144,  49, 157,\n",
       "       182, 331,  17,  77, 495,  40,  96, 133, 334, 111, 360, 102, 222,\n",
       "        59,  58, 456,  69,  56,  15, 272, 181,  93, 175, 557, 492,  29,\n",
       "       187,  47, 467,   4, 356, 359, 282, 344, 109, 530, 371,   8, 149,\n",
       "        71, 536, 150,  27, 385, 452, 279, 463,  30, 367, 161, 415,  20,\n",
       "       137, 118,  31, 188, 363, 405,   7, 491,  26, 186,  23, 142, 235,\n",
       "       200,  85, 409, 365, 327, 140, 280, 502, 488,  79, 329, 458, 125,\n",
       "       402,  65, 440, 346, 151, 273, 296, 121, 407, 103, 455, 459, 159,\n",
       "       362, 218, 145, 354, 425, 480, 558, 416, 384, 152, 531, 183, 406,\n",
       "       424, 460, 236, 550, 328,  35, 428, 490, 441, 156, 529, 475,  13,\n",
       "       560, 196,  57, 171, 239, 509,  53, 227, 122, 447, 126, 192, 167,\n",
       "         5,  52, 136, 120, 523, 535, 201, 169,  48,  61, 454,  63, 129,\n",
       "        50, 487, 461, 540, 114, 130, 443, 351, 204, 524, 101, 119,  64,\n",
       "       163, 532, 208, 539, 113, 197,  24, 271,  82, 412, 146, 561, 564,\n",
       "        92, 445, 391, 390, 533, 128, 199, 358, 373, 508, 395, 191, 112,\n",
       "       357, 414, 319, 135, 106, 194, 537, 124,   1, 174, 556, 368, 464,\n",
       "       566, 260, 481, 442, 525, 514, 567, 339, 397, 568, 510, 469],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = data_nn_dummies_std['end station id']\n",
    "data_nn_dummies_std.set_index('end station id')\n",
    "target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 471 is out of bounds for axis 1 with size 337",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [168], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m target \u001b[39m=\u001b[39m to_categorical(target, num_classes\u001b[39m=\u001b[39m\u001b[39m337\u001b[39m)\n\u001b[0;32m      2\u001b[0m target\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\np_utils.py:73\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     71\u001b[0m n \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m     72\u001b[0m categorical \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((n, num_classes), dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m---> 73\u001b[0m categorical[np\u001b[39m.\u001b[39;49marange(n), y] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     74\u001b[0m output_shape \u001b[39m=\u001b[39m input_shape \u001b[39m+\u001b[39m (num_classes,)\n\u001b[0;32m     75\u001b[0m categorical \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(categorical, output_shape)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 471 is out of bounds for axis 1 with size 337"
     ]
    }
   ],
   "source": [
    "target = to_categorical(target, num_classes=337)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(data_nn_dummies_std)*0.6)\n",
    "split_val = int(len(data_nn_dummies_std)*0.7)\n",
    "x_train = data_nn_dummies_std[:split]\n",
    "y_train = target[:split]\n",
    "y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "x_val = data_nn_dummies_std[split: split_val]\n",
    "y_val = target[split: split_val]\n",
    "y_val = np.asarray(y_val).astype('float32').reshape((-1,1))\n",
    "\n",
    "x_test = data_nn_dummies_std[split_val:]\n",
    "y_test = target[split_val:]\n",
    "y_test = np.asarray(y_test).astype('float32').reshape((-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29476 4912 14739 True\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(x_val) ,len(x_test),len(x_train)+len(x_val)+len(x_test) == len(data_nn_dummies_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'unique'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [158], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m num_features \u001b[39m=\u001b[39m x_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m num_output \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(target\u001b[39m.\u001b[39munique())\n\u001b[0;32m      3\u001b[0m num_output\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unique'"
     ]
    }
   ],
   "source": [
    "num_features = x_train.shape[1]\n",
    "num_output = len(target.unique())\n",
    "num_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(1250, input_dim=num_features, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(750, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(num_output, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 29476\n  y sizes: 16771844\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [160], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCategoricalCrossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[39m# fit the keras model on the dataset\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model\u001b[39m.\u001b[39mfit(x_train, y_train, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m150\u001b[39m, validation_data\u001b[39m=\u001b[39m(x_val,y_val))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\data_adapter.py:1851\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1845\u001b[0m         label,\n\u001b[0;32m   1846\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m   1847\u001b[0m             \u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1848\u001b[0m         ),\n\u001b[0;32m   1849\u001b[0m     )\n\u001b[0;32m   1850\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1851\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 29476\n  y sizes: 16771844\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "model.compile(loss='CategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the keras model on the dataset\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=150, validation_data=(x_val,y_val))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
